{"cells":[{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:21.019525Z","iopub.status.busy":"2024-06-23T13:45:21.018789Z","iopub.status.idle":"2024-06-23T13:45:39.330547Z","shell.execute_reply":"2024-06-23T13:45:39.329700Z","shell.execute_reply.started":"2024-06-23T13:45:21.019490Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset, Dataset, DatasetDict\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline, EvalPrediction\n","from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n","from transformers import EvalPrediction\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["# HateXplain Dataset\n","\n","### Content\n","- **Textual Data**: The dataset contains social media posts from Twitter and Gab.\n","- **Labels**: Each post is labeled as \"normal,\" \"offensive,\" or \"hate,\" indicating the severity of the language.\n","\n","### Annotations\n","- **Label Annotations**: Posts are annotated by multiple human annotators to ensure consistent labeling.\n","- **Rationales**: Annotators provide explanations highlighting specific parts of the text that influenced their labeling decision.\n","- **Target Communities**: Annotations include information on which communities or groups are targeted by the hate speech."]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:39.332887Z","iopub.status.busy":"2024-06-23T13:45:39.332238Z","iopub.status.idle":"2024-06-23T13:45:50.934585Z","shell.execute_reply":"2024-06-23T13:45:50.933683Z","shell.execute_reply.started":"2024-06-23T13:45:39.332858Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n","        num_rows: 15383\n","    })\n","    validation: Dataset({\n","        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n","        num_rows: 1922\n","    })\n","    test: Dataset({\n","        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n","        num_rows: 1924\n","    })\n","})"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["dataset = load_dataset(\"hatexplain\", trust_remote_code=True)\n","dataset"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing\n","- I'll classify both the offensiveness in a ordinal fashion\n","- and the targets of the sentence"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:50.935934Z","iopub.status.busy":"2024-06-23T13:45:50.935655Z","iopub.status.idle":"2024-06-23T13:45:53.185992Z","shell.execute_reply":"2024-06-23T13:45:53.185149Z","shell.execute_reply.started":"2024-06-23T13:45:50.935911Z"},"trusted":true},"outputs":[],"source":["X_train = dataset[\"train\"][\"post_tokens\"]\n","X_val = dataset[\"validation\"][\"post_tokens\"]\n","X_test = dataset[\"test\"][\"post_tokens\"]\n","\n","X_train = [\" \".join(sublist) for sublist in X_train]\n","X_val = [\" \".join(sublist) for sublist in X_val]\n","X_test = [\" \".join(sublist) for sublist in X_test]\n","\n","y_train_lbl = dataset[\"train\"][\"annotators\"]\n","y_val_lbl = dataset[\"validation\"][\"annotators\"]\n","y_test_lbl = dataset[\"test\"][\"annotators\"]\n","\n","y_train_lbl = [el[\"label\"] for el in y_train_lbl]\n","y_val_lbl = [el[\"label\"] for el in y_val_lbl]\n","y_test_lbl = [el[\"label\"] for el in y_test_lbl]\n","\n","y_train_targets = dataset[\"train\"][\"annotators\"]\n","y_val_targets = dataset[\"validation\"][\"annotators\"]\n","y_test_targets = dataset[\"test\"][\"annotators\"]\n","\n","y_train_targets = [el[\"target\"] for el in y_train_targets]\n","y_val_targets = [el[\"target\"] for el in y_val_targets]\n","y_test_targets = [el[\"target\"] for el in y_test_targets]\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.188218Z","iopub.status.busy":"2024-06-23T13:45:53.187919Z","iopub.status.idle":"2024-06-23T13:45:53.194332Z","shell.execute_reply":"2024-06-23T13:45:53.193302Z","shell.execute_reply.started":"2024-06-23T13:45:53.188192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["lesson of the week from the holy book the sovereign individual once commerce flows over the internet suppression of speech becomes unstoppable devolution inevitable and the monopoly of violence by governments will end paraphrased from j d davidson rhys mogg\n","[1, 1, 1]\n","[['None'], ['None'], ['None']]\n","\n","watch bbc on muslims abandoning eu <number> syria the father honestly says it bcs they do not like girls being told they have freewill by supposedly educated pple bcs women don t in islam sharia sharialaw realitycheck fgm misogyny hijab rapeculture abuse violence woman women girl girls womansrights womensrights brexit\n","[1, 1, 1]\n","[['Islam', 'Women'], ['Arab', 'Islam', 'Women'], ['Arab', 'Islam']]\n","\n","<user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires\n","[2, 2, 2]\n","[['Homosexual'], ['Homosexual'], ['Homosexual']]\n","\n","the parade needed at least one severed head in honor of muslim culture\n","[0, 2, 0]\n","[['Islam'], ['Islam'], ['Islam']]\n","\n","i look at the mudslime news from europe and think i wish some goat fucker would try that shit down here in louisiana just get within <number> feet of my daughters and thats your life over checkout time mohammed\n","[0, 0, 0]\n","[['Islam'], ['Men', 'Minority', 'Refugee', 'Arab', 'Islam', 'Heterosexual'], ['Refugee', 'Islam']]\n","\n"]}],"source":["id1 = 140\n","id2 = 145\n","\n","for el1, el2, el3 in zip(X_train[id1:id2],y_train_lbl[id1:id2],y_train_targets[id1:id2]):\n","    print(el1,el2,el3,sep=\"\\n\",end=\"\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Labels\n","\n","I will average and normalize the label values. The actual situation is that for every sentence we have three different annotation which can take the 3 values:\n","- 0 - hatespeech\n","- 1 - normal\n","- 2 - offensive\n","\n","I'll first reorder them, so that\n","\n","- 2 = hate\n","- 1 = offensive\n","- 0 = normal\n","\n","than, since for each sentence there are three different annotation, I'll average them and normalize in the range [0, 1]\n","\n","(e.g., \"they playing a lot of ethnic music at this white ass wedding\" has labels [2, 1, 1] -> [1, 0, 0] -> 0.333 -> 0.165)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.196091Z","iopub.status.busy":"2024-06-23T13:45:53.195762Z","iopub.status.idle":"2024-06-23T13:45:53.219624Z","shell.execute_reply":"2024-06-23T13:45:53.218873Z","shell.execute_reply.started":"2024-06-23T13:45:53.196066Z"},"trusted":true},"outputs":[],"source":["lbl_map = {\n","    0:2,\n","    1:0,\n","    2:1\n","}\n","\n","def avg3(ls):\n","    return (lbl_map[ls[0]]+lbl_map[ls[1]]+lbl_map[ls[2]])/6\n","\n","y_train_lbl = [avg3(x) for x in y_train_lbl]\n","y_val_lbl = [avg3(x) for x in y_val_lbl]\n","y_test_lbl = [avg3(x) for x in y_test_lbl]"]},{"cell_type":"markdown","metadata":{},"source":["I'll also hold a categorical equivalent:\n","- normal  (0 -> 0.33)\n","- offensive (0.33 -> 0.66)\n","- hatespeech  (0.66 -> 1)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.221217Z","iopub.status.busy":"2024-06-23T13:45:53.220795Z","iopub.status.idle":"2024-06-23T13:45:53.234354Z","shell.execute_reply":"2024-06-23T13:45:53.233330Z","shell.execute_reply.started":"2024-06-23T13:45:53.221183Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[2, 2, 1, 1, 1, 0, 2, 0, 0, 2, 1, 0, 0, 0, 2, 1, 2, 2, 1, 0]"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["def ordinalToCategorical (hateScore):\n","    if hateScore < 0.33:\n","        return 0\n","    elif hateScore < 0.66:\n","        return 1\n","    else:\n","        return 2\n","    \n","y_train_lbl_cat = [ordinalToCategorical(i) for i in y_train_lbl]\n","y_val_lbl_cat = [ordinalToCategorical(i) for i in y_val_lbl]\n","y_test_lbl_cat = [ordinalToCategorical(i) for i in y_test_lbl]\n","y_train_lbl_cat[:20]"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Targets\n","\n","A similar procedure should be done also for the targets, If two of the three annotators mentioned the same target I'll maintain it"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.236375Z","iopub.status.busy":"2024-06-23T13:45:53.235785Z","iopub.status.idle":"2024-06-23T13:45:53.243077Z","shell.execute_reply":"2024-06-23T13:45:53.242192Z","shell.execute_reply.started":"2024-06-23T13:45:53.236344Z"},"trusted":true},"outputs":[],"source":["# Done by ChatGPT 4o\n","\n","# Prompt:\n","# Suppose you have three lists of classes,\n","# if the same class is in at least 2 classes\n","# you add it to a new list to return. give me the code\n","\n","from collections import Counter\n","\n","def find_common_classes(list1, list2, list3):\n","    # Combine all lists into one\n","    combined_list = list1 + list2 + list3\n","    \n","    # Create a counter to count occurrences of each element\n","    counter = Counter(combined_list)\n","    \n","    # Create a list to store elements that appear in at least two of the lists\n","    result = [item for item, count in counter.items() if count >= 2]\n","    \n","    return result"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.244500Z","iopub.status.busy":"2024-06-23T13:45:53.244182Z","iopub.status.idle":"2024-06-23T13:45:53.342195Z","shell.execute_reply":"2024-06-23T13:45:53.341215Z","shell.execute_reply.started":"2024-06-23T13:45:53.244475Z"},"trusted":true},"outputs":[],"source":["y_train_targets = [find_common_classes(trgts[0],trgts[1],trgts[2]) for trgts in y_train_targets]\n","y_val_targets = [find_common_classes(trgts[0],trgts[1],trgts[2]) for trgts in y_val_targets]\n","y_test_targets = [find_common_classes(trgts[0],trgts[1],trgts[2]) for trgts in y_test_targets]\n","\n","all_targets = y_train_targets + y_val_targets + y_test_targets"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.343745Z","iopub.status.busy":"2024-06-23T13:45:53.343440Z","iopub.status.idle":"2024-06-23T13:45:53.356485Z","shell.execute_reply":"2024-06-23T13:45:53.355439Z","shell.execute_reply.started":"2024-06-23T13:45:53.343720Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Hindu', 'Indian', 'Asian', 'Economic', 'Disability', 'None', 'Hispanic', 'Islam', 'Refugee', 'Men', 'Other', 'Jewish', 'African', 'Caucasian', 'Buddhism', 'Christian', 'Homosexual', 'Women', 'Indigenous', 'Arab'}\n","20\n"]}],"source":["classes = set()\n","for ls in all_targets:\n","    for el in ls:\n","        classes.add(el)\n","\n","print(classes)\n","print(len(classes))"]},{"cell_type":"markdown","metadata":{},"source":["Removing low incidence groups for tagging"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.359583Z","iopub.status.busy":"2024-06-23T13:45:53.359265Z","iopub.status.idle":"2024-06-23T13:45:53.377165Z","shell.execute_reply":"2024-06-23T13:45:53.376135Z","shell.execute_reply.started":"2024-06-23T13:45:53.359557Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["number of posts where class is mentioned\n","{'None': 6514, 'African': 3166, 'Islam': 2111, 'Jewish': 1925, 'Homosexual': 1837, 'Women': 1534, 'Refugee': 848, 'Other': 755, 'Arab': 753, 'Caucasian': 497, 'Asian': 383, 'Hispanic': 357, 'Men': 84, 'Disability': 54, 'Christian': 45, 'Hindu': 17, 'Indian': 10, 'Economic': 9, 'Buddhism': 2, 'Indigenous': 1}\n","\n","target classes with more than 100 posts\n","{'Other', 'Asian', 'Refugee', 'Jewish', 'Arab', 'Homosexual', 'Women', 'None', 'Hispanic', 'Caucasian', 'African', 'Islam'}\n"]}],"source":["classCounter = dict()\n","\n","for el in classes:\n","    classCounter[el] = 0\n","\n","for ls in all_targets:\n","    for el in ls:\n","        classCounter[el] += 1\n","\n","sorted_classCounter = dict(sorted(classCounter.items(), key=lambda item: item[1], reverse=True))\n","print(\"number of posts where class is mentioned\",sorted_classCounter,sep=\"\\n\",end=\"\\n\\n\")\n","\n","consideredClasses = set()\n","\n","for el in classCounter.items():\n","    if el[1] > 100:\n","        consideredClasses.add(el[0])\n","\n","print(\"target classes with more than 100 posts\",consideredClasses,sep=\"\\n\")"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.378898Z","iopub.status.busy":"2024-06-23T13:45:53.378448Z","iopub.status.idle":"2024-06-23T13:45:53.691071Z","shell.execute_reply":"2024-06-23T13:45:53.690039Z","shell.execute_reply.started":"2024-06-23T13:45:53.378866Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[['Hindu', 'Islam'], ['Refugee', 'Indian'], ['Other', 'Hindu'], ['Hindu'], ['Economic'], ['Hispanic', 'Refugee'], ['Women', 'Disability', 'Islam', 'Homosexual'], ['Hindu', 'Islam'], ['Christian', 'Islam'], ['Refugee']]\n","[['Islam'], ['Refugee'], ['Other'], [], [], ['Hispanic', 'Refugee'], ['Women', 'Islam', 'Homosexual'], ['Islam'], ['Islam'], ['Refugee'], ['Caucasian', 'Women'], ['Caucasian'], ['Caucasian'], ['Caucasian'], ['Refugee'], ['Caucasian'], ['Refugee'], ['African', 'Homosexual'], ['Refugee']]\n"]}],"source":["print(y_train_targets[:10])\n","\n","for i,ls in enumerate(y_train_targets):\n","    y_train_targets[i] = [consClass for consClass in ls if consClass in consideredClasses]\n","\n","for i,ls in enumerate(y_val_targets):\n","    y_val_targets[i] = [consClass for consClass in ls if consClass in consideredClasses]\n","\n","for i,ls in enumerate(y_test_targets):\n","    y_test_targets[i] = [consClass for consClass in ls if consClass in consideredClasses]\n","\n","print(y_train_targets[:19])"]},{"cell_type":"markdown","metadata":{},"source":["This code removes:\n","- entries where the targets appears less than 100 times\n","- entries where all the annotators disagree on the target"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.692558Z","iopub.status.busy":"2024-06-23T13:45:53.692208Z","iopub.status.idle":"2024-06-23T13:45:53.712673Z","shell.execute_reply":"2024-06-23T13:45:53.711720Z","shell.execute_reply.started":"2024-06-23T13:45:53.692532Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["15383 15383\n","14538 14538\n"]}],"source":["print(len(X_train),len(y_train_targets))\n","\n","# By ChatGPT 4o [modified]\n","#\n","# i have a list of sentences and a list of targets\n","# I want to remove each entry if the target ls is empty from both lists, how do I do it?\n","\n","def removeEmptyTargetsEntries(X,y):\n","\n","    # Filter out entries where the target is empty\n","    filtered_pairs = [(s, t) for s, t in zip(X, y) if len(t)!=0]\n","\n","    # Unzip the filtered pairs back into two separate lists\n","    filtered_sentences, filtered_targets = zip(*filtered_pairs) if filtered_pairs else ([], [])\n","\n","    # Convert the tuples back to lists (if needed)\n","    return list(filtered_sentences),list(filtered_targets)\n","\n","X_train_targets,y_train_targets = removeEmptyTargetsEntries(X_train,y_train_targets)\n","X_val_targets,y_val_targets = removeEmptyTargetsEntries(X_val,y_val_targets)\n","X_test_targets,y_test_targets = removeEmptyTargetsEntries(X_test,y_test_targets)\n","\n","\n","print(len(X_train_targets),len(y_train_targets))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.714265Z","iopub.status.busy":"2024-06-23T13:45:53.713948Z","iopub.status.idle":"2024-06-23T13:45:53.749299Z","shell.execute_reply":"2024-06-23T13:45:53.748374Z","shell.execute_reply.started":"2024-06-23T13:45:53.714240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["i live and work with many legal mexican immigrants who are great citizens and trump supporters they have no problem with deporting illegals maga ['Hispanic', 'Refugee']\n","[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n"]}],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","multilabel = MultiLabelBinarizer()\n","\n","y_train_targets_bin = multilabel.fit_transform(y_train_targets).astype('float32')\n","y_val_targets_bin = multilabel.transform(y_val_targets).astype('float32')\n","y_test_targets_bin = multilabel.transform(y_test_targets).astype('float32')\n","\n","print(X_train_targets[3],y_train_targets[3])\n","print(y_train_targets_bin[3])\n"]},{"cell_type":"markdown","metadata":{},"source":["Sample example of the changed dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.750949Z","iopub.status.busy":"2024-06-23T13:45:53.750597Z","iopub.status.idle":"2024-06-23T13:45:53.758378Z","shell.execute_reply":"2024-06-23T13:45:53.757514Z","shell.execute_reply.started":"2024-06-23T13:45:53.750922Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["lesson of the week from the holy book the sovereign individual once commerce flows over the internet suppression of speech becomes unstoppable devolution inevitable and the monopoly of violence by governments will end paraphrased from j d davidson rhys mogg\n","0.0\n","Category: 0\n","['Islam']\n","[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","\n","watch bbc on muslims abandoning eu <number> syria the father honestly says it bcs they do not like girls being told they have freewill by supposedly educated pple bcs women don t in islam sharia sharialaw realitycheck fgm misogyny hijab rapeculture abuse violence woman women girl girls womansrights womensrights brexit\n","0.0\n","Category: 0\n","['Islam', 'Refugee']\n","[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n","\n","<user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires\n","0.5\n","Category: 1\n","['African', 'Homosexual']\n","[1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","\n","the parade needed at least one severed head in honor of muslim culture\n","0.8333333333333334\n","Category: 2\n","['Caucasian']\n","[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","\n","i look at the mudslime news from europe and think i wish some goat fucker would try that shit down here in louisiana just get within <number> feet of my daughters and thats your life over checkout time mohammed\n","1.0\n","Category: 2\n","['Jewish']\n","[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","\n"]}],"source":["id1 = 140\n","id2 = 145\n","\n","for sen, lbl, catlbl, targets, targets_bin in zip(X_train[id1:id2],y_train_lbl[id1:id2],y_train_lbl_cat[id1:id2],y_train_targets[id1:id2],y_train_targets_bin[id1:id2]):\n","    print(sen,lbl,f\"Category: {catlbl}\",targets,targets_bin,end=\"\\n\\n\",sep=\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Preaparing DatasetDict for Bert Models\n","* the first one is for classyfing between 0,1,2 labels\n","* the second one for the target groups"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:45:53.760019Z","iopub.status.busy":"2024-06-23T13:45:53.759711Z","iopub.status.idle":"2024-06-23T13:45:53.819373Z","shell.execute_reply":"2024-06-23T13:45:53.818614Z","shell.execute_reply.started":"2024-06-23T13:45:53.759994Z"},"trusted":true},"outputs":[],"source":["train_data = {'text': X_train, 'label': y_train_lbl_cat}\n","val_data = {'text': X_val, 'label': y_val_lbl_cat}\n","test_data = {'text': X_test, 'label': y_test_lbl_cat}\n","\n","df_train = pd.DataFrame(train_data)\n","df_val = pd.DataFrame(val_data)\n","df_test = pd.DataFrame(test_data)\n","\n","train_dataset = Dataset.from_pandas(df_train)\n","val_dataset = Dataset.from_pandas(df_val)\n","test_dataset = Dataset.from_pandas(df_test)\n","\n","hateXplain = DatasetDict({\n","    \"train\": train_dataset,\n","    \"validation\": val_dataset,\n","    \"test\": test_dataset\n","})"]},{"cell_type":"markdown","metadata":{},"source":["# BERT MODELS"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:14:14.502146Z","iopub.status.busy":"2024-06-23T08:14:14.501787Z","iopub.status.idle":"2024-06-23T08:14:18.814636Z","shell.execute_reply":"2024-06-23T08:14:18.813660Z","shell.execute_reply.started":"2024-06-23T08:14:14.502104Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/antonello03/anaconda3/envs/py31/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Map: 100%|██████████| 15383/15383 [00:01<00:00, 9698.21 examples/s]\n","Map: 100%|██████████| 1922/1922 [00:00<00:00, 21393.83 examples/s]\n","Map: 100%|██████████| 1924/1924 [00:00<00:00, 24357.22 examples/s]"]},{"name":"stdout","output_type":"stream","text":["{'text': 'u really think i would not have been raped by feral hindu or muslim back in india or bangladesh and a neo nazi would rape me as well just to see me cry', 'label': 2, 'input_ids': [101, 1057, 2428, 2228, 1045, 2052, 2025, 2031, 2042, 15504, 2011, 18993, 7560, 2030, 5152, 2067, 1999, 2634, 2030, 7269, 1998, 1037, 9253, 6394, 2052, 9040, 2033, 2004, 2092, 2074, 2000, 2156, 2033, 5390, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model_name = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def tokenize_function(batch):\n","    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n","\n","hateXplain_encoded = hateXplain.map(tokenize_function, batched=True, batch_size=None)\n","\n","print(hateXplain_encoded[\"train\"][0])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:14:18.817359Z","iopub.status.busy":"2024-06-23T08:14:18.816565Z","iopub.status.idle":"2024-06-23T08:14:18.824947Z","shell.execute_reply":"2024-06-23T08:14:18.823346Z","shell.execute_reply.started":"2024-06-23T08:14:18.817323Z"},"trusted":true},"outputs":[],"source":["def get_metrics(preds):\n","  preds_preds = preds.predictions[0] if isinstance(preds.predictions, tuple) else preds.predictions\n","  predictions = preds_preds.argmax(axis=-1)\n","  labels = preds.label_ids\n","\n","  f1 = f1_score(labels, predictions, average='macro')\n","  accuracy = accuracy_score(labels, predictions)\n","  \n","  return {'F1 Score': f1, 'accuracy': accuracy}"]},{"cell_type":"markdown","metadata":{},"source":["## Fine Tuning - {0,1,2} Labels"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:14:18.829796Z","iopub.status.busy":"2024-06-23T08:14:18.829173Z","iopub.status.idle":"2024-06-23T08:14:20.416779Z","shell.execute_reply":"2024-06-23T08:14:20.416002Z","shell.execute_reply.started":"2024-06-23T08:14:18.829743Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d3fdcc29d684d41a54ecf6360523fb4","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["num_labels = 3\n","id2label = {0: \"normal\", 1: \"offensive\", 2: \"hatespeech\"}\n","label2id = {\"normal\": 0, \"offensive\": 1, \"hatespeech\": 2}\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 16\n","epochs = 2\n","logging_steps = len(hateXplain_encoded[\"train\"]) // batch_size\n","model_name_output_dir = model_name.replace(\"/\", \"-\")+\"-finetuned-hateXplain\"\n","training_args_ft = TrainingArguments(output_dir=model_name_output_dir,\n","                                  num_train_epochs=epochs,\n","                                  learning_rate=1e-4,\n","                                  per_device_train_batch_size=batch_size,\n","                                  per_device_eval_batch_size=batch_size,\n","                                  weight_decay=0.01,\n","                                  evaluation_strategy=\"epoch\",\n","                                  disable_tqdm=False,\n","                                  logging_steps=logging_steps,\n","                                  log_level=\"error\",\n","                                  optim='adamw_torch'\n","                                  )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer = Trainer(model=model,\n","                  args=training_args_ft,\n","                  compute_metrics=get_metrics,\n","                  train_dataset=hateXplain_encoded[\"train\"],\n","                  eval_dataset=hateXplain_encoded[\"validation\"],\n","                  tokenizer=tokenizer)\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.save_model()\n","trainer.evaluate()"]},{"cell_type":"markdown","metadata":{},"source":["### Loading and testing"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-hateXplain')\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","batch_size = 16\n","epochs = 2\n","\n","model_name_output_dir = model_name.replace(\"/\", \"-\")+\"-finetuned-hateXplain\"\n","training_args_ft = TrainingArguments(output_dir=model_name_output_dir,\n","                                  num_train_epochs=epochs,\n","                                  learning_rate=1e-4,\n","                                  per_device_train_batch_size=batch_size,\n","                                  per_device_eval_batch_size=batch_size,\n","                                  weight_decay=0.01,\n","                                  evaluation_strategy=\"epoch\",\n","                                  disable_tqdm=False,\n","                                  log_level=\"error\",\n","                                  optim='adamw_torch'\n","                                  )"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'test_loss': 1.5016566514968872, 'test_F1 Score': 0.6319176209758944, 'test_accuracy': 0.6574844074844075, 'test_runtime': 3.9448, 'test_samples_per_second': 487.727, 'test_steps_per_second': 30.673}\n"]}],"source":["model.to(device)\n","trainer = Trainer(model=model, args=training_args_ft, compute_metrics=get_metrics, tokenizer=tokenizer)\n","preds_ft = trainer.predict(hateXplain_encoded['test'])\n","print(preds_ft.metrics)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'label': 'normal', 'score': 0.9897294044494629}]\n","[{'label': 'hatespeech', 'score': 0.600603461265564}]\n","[{'label': 'offensive', 'score': 0.9806431531906128}]\n"]}],"source":["classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n","print(classifier('Bruce lee worst chinese actor'))\n","print(classifier('Bruce lee chinese dog'))\n","print(classifier(\"Imagine having bruce lee in the USA government\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Probing  - {0,1,2} Labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(model_name)\n","num_labels = 3\n","id2label = {0: \"normal\", 1: \"offensive\", 2: \"hatespeech\"}\n","label2id = {\"normal\": 0, \"offensive\": 1, \"hatespeech\": 2}\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for name, param in model.named_parameters():\n","    if 'classifier' not in name:\n","        param.requires_grad = False\n","    else:  # classifier layer\n","        print(name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 16\n","epochs = 5\n","model_name_output_dir = model_name.replace(\"/\", \"-\")+\"-linearprob-hateXplain\"\n","\n","training_args_lp = TrainingArguments(output_dir=model_name_output_dir,\n","                                  num_train_epochs=epochs,\n","                                  learning_rate=1e-4,\n","                                  per_device_train_batch_size=batch_size,\n","                                  per_device_eval_batch_size=batch_size,\n","                                  weight_decay=0.01,\n","                                  evaluation_strategy=\"epoch\",\n","                                  disable_tqdm=False,\n","                                  log_level=\"error\",\n","                                  optim='adamw_torch'\n","                                  )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer = Trainer(model=model,\n","                  args=training_args_lp,\n","                  compute_metrics=get_F1,\n","                  train_dataset=hateXplain_encoded[\"train\"],\n","                  eval_dataset=hateXplain_encoded[\"validation\"],\n","                  tokenizer=tokenizer)\n","trainer.train()\n","trainer.save_model()"]},{"cell_type":"markdown","metadata":{},"source":["### Loading and Testing - Linear Prob"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/antonello03/anaconda3/envs/py31/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-linearprob-hateXplain')\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.8942822813987732,\n"," 'eval_F1 Score': 0.479689145518902,\n"," 'eval_accuracy': 0.5962539021852237,\n"," 'eval_runtime': 4.2041,\n"," 'eval_samples_per_second': 457.175,\n"," 'eval_steps_per_second': 28.782}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["training_args_lp = TrainingArguments(output_dir=model_name_output_dir,\n","                                  num_train_epochs=epochs,\n","                                  learning_rate=1e-4,\n","                                  per_device_train_batch_size=batch_size,\n","                                  per_device_eval_batch_size=batch_size,\n","                                  weight_decay=0.01,\n","                                  evaluation_strategy=\"epoch\",\n","                                  disable_tqdm=False,\n","                                  log_level=\"error\",\n","                                  optim='adamw_torch'\n","                                  )\n","trainer = Trainer(model=model, args=training_args_lp, compute_metrics=get_metrics, tokenizer=tokenizer, eval_dataset=hateXplain_encoded[\"validation\"])\n","trainer.evaluate()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'test_loss': 0.9055814146995544, 'test_F1 Score': 0.4776900138139939, 'test_accuracy': 0.5883575883575883, 'test_runtime': 3.97, 'test_samples_per_second': 484.639, 'test_steps_per_second': 30.479}\n"]}],"source":["model.to(device)\n","preds_probing = trainer.predict(hateXplain_encoded['test'])\n","print(preds_probing.metrics)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'label': 'normal', 'score': 0.8035374283790588}]\n","[{'label': 'hatespeech', 'score': 0.6116581559181213}]\n","[{'label': 'normal', 'score': 0.45415931940078735}]\n"]}],"source":["classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n","print(classifier('Bruce lee worst chinese actor'))\n","print(classifier('Bruce lee chinese dog'))\n","print(classifier(\"Imagine having bruce lee in the USA government\"))"]},{"cell_type":"markdown","metadata":{},"source":["# Group Tagging with BERT - only finetuning"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing Steps"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T13:28:36.506191Z","iopub.status.busy":"2024-06-18T13:28:36.505506Z","iopub.status.idle":"2024-06-18T13:28:36.513536Z","shell.execute_reply":"2024-06-18T13:28:36.512515Z","shell.execute_reply.started":"2024-06-18T13:28:36.506160Z"},"trusted":true},"outputs":[],"source":["# Lets build custom dataset\n","class CustomDataset(Dataset):\n","  def __init__(self, texts, labels, tokenizer, max_len=128):\n","    self.texts = texts\n","    self.labels = labels\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    return len(self.texts)\n","\n","  def __getitem__(self, idx):\n","    text = str(self.texts[idx])\n","    label = torch.tensor(self.labels[idx])\n","\n","    encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n","\n","    return {\n","        'input_ids': encoding['input_ids'].flatten(),\n","        'attention_mask': encoding['attention_mask'].flatten(),\n","        'labels': label\n","    }\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["train_ds_trgt = CustomDataset(X_train_targets, y_train_targets_bin, tokenizer)\n","val_ds_trgt = CustomDataset(X_val_targets, y_val_targets_bin, tokenizer)\n","test_ds_trgt = CustomDataset(X_test_targets, y_test_targets_bin, tokenizer)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["#CHAT GPT after providing codes errors\n","\n","def convert_to_hf_dataset(custom_dataset):\n","    data = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'labels': []\n","    }\n","    \n","    for i in range(len(custom_dataset)):\n","        item = custom_dataset[i]\n","        data['input_ids'].append(item['input_ids'].numpy())\n","        data['attention_mask'].append(item['attention_mask'].numpy())\n","        data['labels'].append(item['labels'].numpy())\n","        \n","    hf_dataset = Dataset.from_dict(data)\n","    return hf_dataset\n","\n","\n","train_ds_trgt = convert_to_hf_dataset(train_ds_trgt)\n","val_ds_trgt = convert_to_hf_dataset(val_ds_trgt)\n","test_ds_trgt = convert_to_hf_dataset(test_ds_trgt)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Multi-Label Classification Evaluation Metrics\n","def multi_labels_metrics(predictions, labels, threshold=0.3):\n","  sigmoid = torch.nn.Sigmoid()\n","  probs = sigmoid(torch.Tensor(predictions))\n","\n","  y_pred = np.zeros(probs.shape)\n","  y_pred[np.where(probs>=threshold)] = 1\n","  y_true = labels\n","\n","  accuracy = accuracy_score(y_true, y_pred)\n","  f1 = f1_score(y_true, y_pred, average = 'macro')\n","  roc_auc = roc_auc_score(y_true, y_pred, average = 'macro')\n","\n","  metrics = {\n","      \"roc_auc\": roc_auc,\n","      \"f1\": f1,\n","      \"accuracy\": accuracy\n","  }\n","\n","  return metrics\n","\n","def compute_metrics(p:EvalPrediction):\n","  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","\n","  result = multi_labels_metrics(predictions=preds,\n","                                labels=p.label_ids)\n","\n","  return result"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n","model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(consideredClasses),\n","                                                            problem_type=\"multi_label_classification\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training Arguments\n","from transformers import TrainingArguments, Trainer\n","\n","model_name_output_dir = checkpoint.replace(\"/\", \"-\")+\"-targets-ft-hateXplain\"\n","\n","args = TrainingArguments(\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    output_dir = model_name_output_dir,\n","    num_train_epochs=5,\n","    save_steps=1000,\n","    save_total_limit=2,\n","    evaluation_strategy=\"epoch\"\n",")\n","\n","trainer = Trainer(model=model,\n","                  args=args,\n","                  train_dataset = train_ds_trgt,\n","                  eval_dataset = val_ds_trgt,\n","                  compute_metrics=compute_metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.save_model()"]},{"cell_type":"markdown","metadata":{},"source":["## Loading and Testing"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/antonello03/anaconda3/envs/py31/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-targets-ft-hateXplain\"\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(consideredClasses),\n","                                                            problem_type=\"multi_label_classification\")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.23712904751300812, 'eval_roc_auc': 0.8423115948168248, 'eval_f1': 0.6765660930808567, 'eval_accuracy': 0.6360619469026548, 'eval_runtime': 5.5255, 'eval_samples_per_second': 327.212, 'eval_steps_per_second': 40.901}\n"]},{"data":{"text/plain":["{'eval_loss': 0.23712904751300812,\n"," 'eval_roc_auc': 0.8423115948168248,\n"," 'eval_f1': 0.6765660930808567,\n"," 'eval_accuracy': 0.6360619469026548,\n"," 'eval_runtime': 5.5255,\n"," 'eval_samples_per_second': 327.212,\n"," 'eval_steps_per_second': 40.901}"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["args = TrainingArguments(per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    output_dir = checkpoint,\n","    num_train_epochs=5,\n","    evaluation_strategy=\"epoch\"\n",")\n","\n","trainer = Trainer(model=model,\n","                  args=args,\n","                  train_dataset = train_ds_trgt,\n","                  eval_dataset = val_ds_trgt,\n","                  compute_metrics=compute_metrics)\n","\n","trainer.evaluate()"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'test_loss': 0.22575204074382782, 'test_roc_auc': 0.8437021346529375, 'test_f1': 0.6968215778219594, 'test_accuracy': 0.6524153248195447, 'test_runtime': 5.3313, 'test_samples_per_second': 337.813, 'test_steps_per_second': 42.391}\n"]}],"source":["preds_targets = trainer.predict(test_ds_trgt)\n","print(preds_targets.metrics)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["always thought that nigger was a faggot ['African', 'Homosexual']\n"]}],"source":["text = X_test_targets[50]\n","print(text,y_test_targets[50])"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["[('African', 'Homosexual')]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["encoding = tokenizer(text, return_tensors='pt')\n","encoding.to(trainer.model.device)\n","outputs = trainer.model(**encoding)\n","\n","sigmoid = torch.nn.Sigmoid()\n","probs = sigmoid(outputs.logits[0].cpu())\n","preds = np.zeros(probs.shape)\n","preds[np.where(probs>=0.3)] = 1\n","\n","multilabel.classes_\n","\n","multilabel.inverse_transform(preds.reshape(1,-1))"]},{"cell_type":"markdown","metadata":{},"source":["# BiRNN and simple TL-IDF linear regression"]},{"cell_type":"markdown","metadata":{},"source":["Use simple nltk tokenization"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:46:05.424593Z","iopub.status.busy":"2024-06-23T13:46:05.424227Z","iopub.status.idle":"2024-06-23T13:46:06.018988Z","shell.execute_reply":"2024-06-23T13:46:06.018099Z","shell.execute_reply.started":"2024-06-23T13:46:05.424566Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /home/antonello03/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/antonello03/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["#let's create our tokenizer function to tokenize the sentences\n","import nltk\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","import string\n","punctuations = list(string.punctuation)\n","\n","stopwords_list = list(stopwords.words('english'))\n","\n","\n","def nltk_tokenizer(sentence):\n","    #we lowercase all sentences\n","    sentence = sentence.lower()\n","\n","    #here we tokenize it using nltk\n","    my_tokenized_tokens = word_tokenize(sentence)\n","\n","    # Removing stop words and punctuations\n","    mytokens = [word for word in my_tokenized_tokens if word not in stopwords_list and word not in punctuations]\n","\n","    # return preprocessed list of tokens\n","    return mytokens"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:46:07.335829Z","iopub.status.busy":"2024-06-23T13:46:07.335189Z","iopub.status.idle":"2024-06-23T13:46:12.625842Z","shell.execute_reply":"2024-06-23T13:46:12.624861Z","shell.execute_reply.started":"2024-06-23T13:46:07.335797Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["apply tokenizer to test data\n","apply tokenizer to train data\n"]}],"source":["#tokenize train/test data\n","print(\"apply tokenizer to test data\")\n","df_test['tokens'] = df_test['text'].apply(nltk_tokenizer)\n","df_test['sentence'] =  df_test.tokens.apply(lambda x: ' '.join(x))\n","\n","\n","print(\"apply tokenizer to train data\")\n","df_train['tokens'] = df_train['text'].apply(nltk_tokenizer)\n","df_train['sentence'] =  df_train.tokens.apply(lambda x: ' '.join(x))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:46:12.628002Z","iopub.status.busy":"2024-06-23T13:46:12.627725Z","iopub.status.idle":"2024-06-23T13:46:12.716516Z","shell.execute_reply":"2024-06-23T13:46:12.715599Z","shell.execute_reply.started":"2024-06-23T13:46:12.627978Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["24795\n"]}],"source":["#first we need to define the vocabulary using the training data only\n","vocab = set()\n","for sent in df_train['sentence']:\n","    for word in sent.split(\" \"):\n","        vocab.add(word.strip())\n","\n","#print(vocab)\n","print(len(vocab))"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:46:12.717979Z","iopub.status.busy":"2024-06-23T13:46:12.717695Z","iopub.status.idle":"2024-06-23T13:46:13.147127Z","shell.execute_reply":"2024-06-23T13:46:13.146312Z","shell.execute_reply.started":"2024-06-23T13:46:12.717955Z"},"trusted":true},"outputs":[],"source":["documents_train = list(df_train.sentence)\n","documents_test = list(df_test.sentence)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","#tfidf for the training data\n","vectorizer = CountVectorizer(lowercase=True, vocabulary=vocab)\n","X_count_train = vectorizer.fit_transform(documents_train)\n","transformer = TfidfTransformer()\n","X_tfidf_train = transformer.fit_transform(X_count_train)\n","\n","#tfidf for the testing data\n","vectorizer = CountVectorizer(lowercase=True, vocabulary=vocab)\n","X_count_test = vectorizer.fit_transform(documents_test)\n","transformer = TfidfTransformer()\n","X_tfidf_test = transformer.fit_transform(X_count_test)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:46:13.149287Z","iopub.status.busy":"2024-06-23T13:46:13.149008Z","iopub.status.idle":"2024-06-23T13:46:17.657001Z","shell.execute_reply":"2024-06-23T13:46:17.655711Z","shell.execute_reply.started":"2024-06-23T13:46:13.149262Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression Accuracy: 0.6522869022869023\n","Logistic F1 macro: 0.6016885518340791\n","Logistic ROC AUC : 0.7975026994533584\n"]}],"source":["Y_label_train = df_train['label'].to_list()\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","\n","classifier = LogisticRegression(max_iter = 1000)\n","classifier.fit(X_tfidf_train, Y_label_train)\n","\n","\n","y_test = df_test['label'].to_list()\n","predicted = classifier.predict(X_tfidf_test)\n","predicted_prob = classifier.predict_proba(X_tfidf_test)\n","print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted))\n","print(\"Logistic F1 macro:\", metrics.f1_score(y_test, predicted, average = \"macro\"))\n","print(\"Logistic ROC AUC :\", metrics.roc_auc_score(y_test, predicted_prob, multi_class='ovr'))"]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained Glove embedding and BiRNN for target and class prediction"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T13:46:17.660192Z","iopub.status.busy":"2024-06-23T13:46:17.659484Z"},"trusted":true},"outputs":[],"source":["import gensim.downloader\n","pretraines_glove_model = gensim.downloader.load('glove-wiki-gigaword-100')"]},{"cell_type":"markdown","metadata":{},"source":["Now create the embeddings"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:10:53.312875Z","iopub.status.busy":"2024-06-23T14:10:53.312130Z","iopub.status.idle":"2024-06-23T14:10:53.346722Z","shell.execute_reply":"2024-06-23T14:10:53.345450Z","shell.execute_reply.started":"2024-06-23T14:10:53.312837Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('terror', 0.8976657390594482),\n"," ('terrorists', 0.8589985966682434),\n"," ('terrorism', 0.8219908475875854),\n"," ('attacks', 0.8140439391136169),\n"," ('qaida', 0.7818638682365417),\n"," ('qaeda', 0.7712634205818176),\n"," ('bombings', 0.7330332398414612),\n"," ('extremist', 0.7313344478607178),\n"," ('militant', 0.7306753396987915),\n"," ('suspected', 0.7263491153717041)]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["pretraines_glove_model.most_similar('terrorist')"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:10:39.912457Z","iopub.status.busy":"2024-06-23T14:10:39.911621Z","iopub.status.idle":"2024-06-23T14:10:39.923915Z","shell.execute_reply":"2024-06-23T14:10:39.922914Z","shell.execute_reply.started":"2024-06-23T14:10:39.912426Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# set to collect unkown words for whic the embeddings is a null vector\n","unkown_words = set()\n","\n","    \n","def get_word_embedding(emb_model,word, emb_dim):\n","\n","  if word in emb_model: #wv\n","        return emb_model[word]\n","  else:\n","        global unkown_words\n","        unkown_words.add(word)\n","\n","        return np.zeros(emb_dim)  # For unknown words\n","\n","\n","def pad_sequence(embeddings, max_length, embedding_dim):\n","    if len(embeddings) < max_length:\n","        padding = np.zeros((max_length - len(embeddings), embedding_dim))\n","        embeddings = np.vstack((embeddings, padding))\n","    else:\n","        embeddings = embeddings[:max_length]\n","    return embeddings\n","\n","\n","def create_embedding_vectors(emb_model, df_train, df_test, emb_dim):\n","    \n","    global unkown_words\n","    \n","    # decide padding dim according to longest sentence in df_train\n","    max_length = max(len(sentence) for sentence in df_train['tokens'])\n","    print(f\"max lenght: {max_length}\")\n","    \n","    \n","    embedded_sentences = []\n","    \n","    for sentence in df_train['tokens']:\n","        words = [word for word in sentence]\n","\n","        embeddings = [get_word_embedding(emb_model,word,emb_dim) for word in words]\n","        padded_embeddings = pad_sequence(embeddings, max_length, emb_dim)\n","        embedded_sentences.append(padded_embeddings)\n","\n","    embedded_sentences_test = []\n","    for sentence in df_test['tokens']:\n","      words = [word for word in sentence]\n","\n","      embeddings = [get_word_embedding(emb_model,word,emb_dim) for word in words]\n","      padded_embeddings = pad_sequence(embeddings, max_length, emb_dim)\n","      embedded_sentences_test.append(padded_embeddings)\n","\n","\n","    print(f\" words not found: {len(unkown_words)}\")\n","    \n","    \n","    # Organize data\n","    data_test = list(zip(np.array(embedded_sentences_test), df_test[\"label\"].to_numpy()))\n","    data_train = list(zip(np.array(embedded_sentences), df_train[\"label\"].to_numpy()))\n","    \n","    return data_train, data_test"]},{"cell_type":"markdown","metadata":{},"source":["## Now we create the data tuple (X,label) for target and hate speech classification\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:10:42.213640Z","iopub.status.busy":"2024-06-23T14:10:42.213010Z","iopub.status.idle":"2024-06-23T14:10:52.307349Z","shell.execute_reply":"2024-06-23T14:10:52.306489Z","shell.execute_reply.started":"2024-06-23T14:10:42.213611Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["max lenght: 144\n"," words not found: 5137\n","max lenght: 144\n"," words not found: 5137\n"]}],"source":["train_target_data = {'text':X_train_targets, 'label':y_train_targets_bin.tolist() }\n","test_target_data = {'text':X_test_targets, 'label':y_test_targets_bin.tolist() }\n","\n","df_train_target = pd.DataFrame(train_target_data)\n","df_test_target  = pd.DataFrame(test_target_data )\n","\n","\n","df_train_target['tokens'] = df_train_target['text'].apply(nltk_tokenizer)\n","df_train_target['sentence'] =  df_train_target.tokens.apply(lambda x: ' '.join(x))\n","\n","df_test_target['tokens'] = df_test_target['text'].apply(nltk_tokenizer)\n","df_test_target['sentence'] =  df_test_target.tokens.apply(lambda x: ' '.join(x))\n","\n","data_train, data_test = create_embedding_vectors(emb_model = pretraines_glove_model, df_train = df_train, df_test = df_test, emb_dim = pretraines_glove_model.vector_size)\n","\n","data_train_targets, data_test_targets = create_embedding_vectors(emb_model = pretraines_glove_model, df_train = df_train_target, df_test = df_test_target, emb_dim = pretraines_glove_model.vector_size)"]},{"cell_type":"markdown","metadata":{},"source":["Now we will create a custom dataset to train the birnn pytorch model for text classification"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:10:57.702933Z","iopub.status.busy":"2024-06-23T14:10:57.702054Z","iopub.status.idle":"2024-06-23T14:10:57.715849Z","shell.execute_reply":"2024-06-23T14:10:57.714830Z","shell.execute_reply.started":"2024-06-23T14:10:57.702899Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","\n","class GloveDataset(Dataset):\n","    def __init__(self, data):\n","        self.dataset = data\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        embeddings, label = self.dataset[idx]\n","        embeddings = torch.tensor(embeddings, dtype=torch.float32)\n","        label = torch.tensor(label, dtype=torch.long)\n","        embeddings = embeddings.to(device)\n","        label = label.to(device)\n","        return embeddings, label\n","\n","class GloveDataset_target(Dataset):\n","    def __init__(self, data):\n","        self.dataset = data\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        embeddings, label = self.dataset[idx]\n","        embeddings = torch.tensor(embeddings, dtype=torch.float32)\n","        label = torch.tensor(label, dtype=torch.float32)\n","        embeddings = embeddings.to(device)\n","        label = label.to(device)\n","        return embeddings, label\n","\n","\n","\n","batch_size = 16\n","\n","# hate_speech classification dataloaders\n","dataset = GloveDataset(data_train)\n","dataloader_glove_train = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataset = GloveDataset(data_test)\n","dataloader_glove_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# target classification dataloaders\n","\n","dataset_target = GloveDataset_target(data_train_targets)\n","dataloader_glove_target_train = DataLoader(dataset_target, batch_size=batch_size, shuffle=True)\n","\n","test_dataset_target = GloveDataset_target(data_test_targets)\n","dataloader_glove_target_test = DataLoader(test_dataset_target, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Generic readapted BiRNN model\n","source: https://github.com/hate-alert/HateXplain/blob/master/Models/otherModels.py"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:11:00.196200Z","iopub.status.busy":"2024-06-23T14:11:00.195533Z","iopub.status.idle":"2024-06-23T14:11:00.209856Z","shell.execute_reply":"2024-06-23T14:11:00.208916Z","shell.execute_reply.started":"2024-06-23T14:11:00.196168Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","\n","class BiRNN(nn.Module):  \n","    def __init__(self,args):\n","        super(BiRNN, self).__init__()\n","        \n","        self.hidden_size = args['hidden_size']\n","        self.batch_size = args['batch_size']\n","        self.drop_embed=args['drop_embed']\n","        self.drop_fc=args['drop_fc']\n","        self.drop_hidden=args['drop_hidden']\n","        self.seq_model_name=args[\"seq_model\"]\n","        self.embedsize=args[\"embed_size\"]\n","        self.num_layers = args[\"num_layers\"]\n","  \n","       \n","\n","        \n","        if(args[\"seq_model\"]==\"lstm\"):\n","            self.seq_model = nn.LSTM(args[\"embed_size\"], self.hidden_size,num_layers =self.num_layers, bidirectional=True, batch_first=True,dropout=self.drop_hidden)\n","        elif(args[\"seq_model\"]==\"gru\"):\n","            self.seq_model = nn.GRU(args[\"embed_size\"], self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True,dropout=self.drop_hidden) \n","            \n","        self.linear1 = nn.Linear(2 * self.hidden_size*self.num_layers, self.hidden_size)\n","        self.linear2 = nn.Linear(self.hidden_size, args['num_classes'])\n","        self.dropout_embed = nn.Dropout2d(self.drop_embed)\n","        self.dropout_fc = nn.Dropout(self.drop_fc)\n","        self.num_labels=args['num_classes']\n","        \n","        \n","        \n","    def forward(self,X):\n","        batch_size = X.size(0)\n","        h_embedding = torch.squeeze(self.dropout_embed(torch.unsqueeze(X, 0))).view(batch_size, X.shape[1], self.embedsize)\n","        \n","        # Forward propagate through LSTM/GRU\n","        if self.seq_model_name == \"lstm\":\n","            _, hidden = self.seq_model(h_embedding)\n","            hidden = hidden[0]\n","        else:\n","            _, hidden = self.seq_model(h_embedding)\n","\n","       \n","     \n","        hidden = hidden.transpose(0, 1).contiguous().view(X.size(0), -1) \n","        hidden = self.dropout_fc(hidden)\n","        hidden = torch.relu(self.linear1(hidden))  #batch x hidden_size\n","        hidden = self.dropout_fc(hidden)\n","        logits = self.linear2(hidden)\n","        \n","        return (logits)\n","    \n","    \n","    \n","    def init_hidden(self, batch_size):\n","        return cuda_available(torch.zeros(2, self.batch_size, self.hidden_size))\n","    \n","\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:11:02.560018Z","iopub.status.busy":"2024-06-23T14:11:02.559686Z","iopub.status.idle":"2024-06-23T14:11:02.947331Z","shell.execute_reply":"2024-06-23T14:11:02.946383Z","shell.execute_reply.started":"2024-06-23T14:11:02.559994Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BiRNN(\n","  (seq_model): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n","  (linear1): Linear(in_features=1536, out_features=256, bias=True)\n","  (linear2): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout_embed): Dropout2d(p=0.1, inplace=False)\n","  (dropout_fc): Dropout(p=0.1, inplace=False)\n",")"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["# model for hatespeech classification\n","args_dict_classification_hate = {\n","        \"batch_size\":16,\n","        \"hidden_size\":256,\n","        \"embed_size\":100,\n","        \"num_classes\" : 3,\n","        \"num_layers\":3,\n","        \"drop\":0.1,\n","        \"learning_rate\":0.001,\n","        \"seq_model\":\"lstm\",\n","        \"drop_embed\":0.1,\n","        \"drop_fc\":0.1,\n","        \"drop_hidden\":0.1,\n","        }\n","    \n","BiRNN_for_hate_class = BiRNN(args_dict_classification_hate)\n","BiRNN_for_hate_class.to(device)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:11:04.498975Z","iopub.status.busy":"2024-06-23T14:11:04.498159Z","iopub.status.idle":"2024-06-23T14:11:04.570590Z","shell.execute_reply":"2024-06-23T14:11:04.569707Z","shell.execute_reply.started":"2024-06-23T14:11:04.498946Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BiRNN(\n","  (seq_model): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n","  (linear1): Linear(in_features=1536, out_features=256, bias=True)\n","  (linear2): Linear(in_features=256, out_features=12, bias=True)\n","  (dropout_embed): Dropout2d(p=0.1, inplace=False)\n","  (dropout_fc): Dropout(p=0.1, inplace=False)\n",")"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["#model for target classification \n","\n","n_classes = len(y_train_targets_bin[0])\n","args_dict_classification_target = {\n","        \"batch_size\":16,\n","        \"hidden_size\":256,\n","        \"embed_size\":100,\n","        \"num_classes\" : n_classes ,\n","        \"num_layers\":3,\n","        \"drop\":0.1,\n","        \"learning_rate\":0.001,\n","        \"seq_model\":\"lstm\",\n","        \"drop_embed\":0.1,\n","        \"drop_fc\":0.1,\n","        \"drop_hidden\":0.1,\n","        }\n","    \n","BiRNN_for_target_class = BiRNN(args_dict_classification_target)\n","BiRNN_for_target_class.to(device)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:11:06.109190Z","iopub.status.busy":"2024-06-23T14:11:06.108351Z","iopub.status.idle":"2024-06-23T14:11:06.122215Z","shell.execute_reply":"2024-06-23T14:11:06.121249Z","shell.execute_reply.started":"2024-06-23T14:11:06.109156Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","from sklearn import metrics\n","import torch.nn.functional as F\n","\n","def calculate_metrics(preds, labels, task):\n","    # preds are softmax if task is hate_speech otherwise sigmoid\n","    if task == 'hate_speech':\n","        preds_label = np.argmax(preds, axis=1)\n","    elif task == 'target_clf':\n","        preds = torch.sigmoid(torch.tensor(preds)).numpy()\n","        preds_label = (preds > 0.5).astype(int)\n","    \n","    else:\n","        raise ValueError(\"please provde a valid task between ['hate_speech', 'target_clf']\")\n","    \n","    \n","    accuracy = metrics.accuracy_score(labels, preds_label)\n","    macro_f1 = metrics.f1_score(labels, preds_label, average = \"macro\")\n","    auroc = metrics.roc_auc_score(labels, preds, multi_class='ovr')\n","    \n","    return accuracy, macro_f1, auroc\n","\n","\n","def custom_trainer(model, dataloader, num_epochs, criterion, optimizer, task = None):\n","    \n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for embeddings, labels in dataloader:\n","            # Move tensors to the configured device\n","            \n","            embeddings = embeddings.to(device)\n","            labels = labels.to(device)\n","            \n","\n","            # Forward pass\n","            outputs = model(embeddings)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        avg_loss = running_loss / len(dataloader)\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n","\n","        # Validation step\n","        model.eval()\n","        all_preds = []\n","        all_labels = []\n","        with torch.no_grad():\n","            for embeddings, labels in dataloader:\n","                embeddings = embeddings.to(device)\n","                labels = labels.to(device)\n","                outputs = model(embeddings) \n","                \n","                if task == 'hate_speech':\n","                    outputs = F.softmax(outputs, dim=1)\n","                \n","                    \n","                all_preds.extend(outputs.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        accuracy, macro_f1, auroc = calculate_metrics(all_preds, all_labels,task=task)\n","        print(f'Accuracy: {accuracy:.4f}, Macro F1: {macro_f1:.4f},AUROC: {auroc:.4f}')\n","    \n","    return model\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### Train the hate speech classifier"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:11:31.797610Z","iopub.status.busy":"2024-06-23T14:11:31.797248Z","iopub.status.idle":"2024-06-23T14:16:19.695445Z","shell.execute_reply":"2024-06-23T14:16:19.694431Z","shell.execute_reply.started":"2024-06-23T14:11:31.797579Z"},"trusted":true},"outputs":[],"source":["from torch import optim\n","from torch import nn\n","\n","criterion = criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(BiRNN_for_hate_class.parameters(), lr=0.001)\n","\n","BiRNN_for_hate_class = custom_trainer(BiRNN_for_hate_class, dataloader_glove_train, 5,criterion, optimizer, task = 'hate_speech')"]},{"cell_type":"markdown","metadata":{},"source":["### Train target classifier \n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:16:19.697348Z","iopub.status.busy":"2024-06-23T14:16:19.697072Z","iopub.status.idle":"2024-06-23T14:20:57.717936Z","shell.execute_reply":"2024-06-23T14:20:57.717003Z","shell.execute_reply.started":"2024-06-23T14:16:19.697323Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/5], Loss: 0.2080\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_35/3958561599.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n","  preds = torch.sigmoid(torch.tensor(preds)).numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.5008, Macro F1: 0.4055,AUROC: 0.8893\n","Epoch [2/5], Loss: 0.1696\n","Accuracy: 0.5598, Macro F1: 0.5596,AUROC: 0.9274\n","Epoch [3/5], Loss: 0.1536\n","Accuracy: 0.6336, Macro F1: 0.5920,AUROC: 0.9460\n","Epoch [4/5], Loss: 0.1437\n","Accuracy: 0.6502, Macro F1: 0.6838,AUROC: 0.9545\n","Epoch [5/5], Loss: 0.1338\n","Accuracy: 0.7121, Macro F1: 0.6670,AUROC: 0.9694\n"]}],"source":["from torch import optim\n","from torch import nn\n","\n","criterion = nn.BCEWithLogitsLoss() # combines a sigmoid layer and the binary cross-entropy loss in a single class for a good multi-class multi target classification loss\n","optimizer = optim.Adam(BiRNN_for_target_class.parameters(), lr=0.001)\n","\n","BiRNN_for_target_class = custom_trainer(BiRNN_for_target_class, dataloader_glove_target_train, 5, criterion, optimizer, task = 'target_clf')"]},{"cell_type":"markdown","metadata":{},"source":["## Test and final pipeline"]},{"cell_type":"markdown","metadata":{},"source":["**Load models to avoid training**"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["Load = False\n","\n","# remove comment to load the pretrained models\n","Load = True\n","\n","if Load:\n","    BiRNN_for_target_class_path = 'BiRNN_for_target_class.pth'\n","    BiRNN_for_hate_class_path = 'BiRNN_for_hate_class.pth'\n","\n","    # Load the state dictionaries\n","    BiRNN_for_target_class = torch.load(BiRNN_for_target_class_path)\n","    BiRNN_for_hate_class = torch.load(BiRNN_for_hate_class_path)\n"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:21:04.080818Z","iopub.status.busy":"2024-06-23T14:21:04.080104Z","iopub.status.idle":"2024-06-23T14:21:04.088111Z","shell.execute_reply":"2024-06-23T14:21:04.087232Z","shell.execute_reply.started":"2024-06-23T14:21:04.080790Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, dataloader,task):# mode eval\n","    \n","    model.to(device)\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for embeddings, labels in dataloader:\n","            embeddings = embeddings.to(device)\n","            labels = labels.to(device)\n","            outputs = model(embeddings) \n","                \n","            if task == 'hate_speech':\n","                outputs = F.softmax(outputs, dim=1)\n","                \n","                    \n","            all_preds.extend(outputs.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy, macro_f1, auroc = calculate_metrics(all_preds, all_labels,task=task)\n","    print(f'Accuracy: {accuracy:.4f}, Macro F1: {macro_f1:.4f},AUROC: {auroc:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["### Test Hate classification task"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:21:06.554151Z","iopub.status.busy":"2024-06-23T14:21:06.553434Z","iopub.status.idle":"2024-06-23T14:21:08.655490Z","shell.execute_reply":"2024-06-23T14:21:08.654459Z","shell.execute_reply.started":"2024-06-23T14:21:06.554118Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6377, Macro F1: 0.5864,AUROC: 0.7824\n"]}],"source":["evaluate_model(BiRNN_for_hate_class, dataloader_glove_test, task =  'hate_speech')"]},{"cell_type":"markdown","metadata":{},"source":["### Test Target classification task"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:21:09.941618Z","iopub.status.busy":"2024-06-23T14:21:09.941284Z","iopub.status.idle":"2024-06-23T14:21:11.776697Z","shell.execute_reply":"2024-06-23T14:21:11.775647Z","shell.execute_reply.started":"2024-06-23T14:21:09.941591Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6013, Macro F1: 0.5789,AUROC: 0.9241\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_40368/3207407138.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n","  preds = torch.sigmoid(torch.tensor(preds)).numpy()\n"]}],"source":["evaluate_model(BiRNN_for_target_class, dataloader_glove_target_test, task = 'target_clf')"]},{"cell_type":"markdown","metadata":{},"source":["save model"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T14:21:15.677540Z","iopub.status.busy":"2024-06-23T14:21:15.676935Z","iopub.status.idle":"2024-06-23T14:21:15.761050Z","shell.execute_reply":"2024-06-23T14:21:15.760059Z","shell.execute_reply.started":"2024-06-23T14:21:15.677509Z"},"trusted":true},"outputs":[],"source":["torch.save(BiRNN_for_hate_class, 'BiRNN_for_hate_class.pth')\n","torch.save(BiRNN_for_target_class, 'BiRNN_for_target_class.pth')"]},{"cell_type":"markdown","metadata":{},"source":["## Final wrapper for custom input \n","\n","Output hate score and targets"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T10:28:06.735196Z","iopub.status.busy":"2024-06-23T10:28:06.734774Z","iopub.status.idle":"2024-06-23T10:28:06.760084Z","shell.execute_reply":"2024-06-23T10:28:06.758894Z","shell.execute_reply.started":"2024-06-23T10:28:06.735164Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Hate score: normal\n","Probability: 0.40891650319099426\n","Targets: [('None',)]\n"]}],"source":["def predict(text, hate_model, target_model, emb_model, emb_dim = 100):\n","    # Tokenize the input text\n","    max_len=len(text)\n","    tokens = nltk_tokenizer(text)\n","    predict_embeddings = []\n","    emb_list = []\n","    \n","    embeddings = [get_word_embedding(emb_model,token,emb_dim) for token in tokens] \n","    \n","        \n","    padded_embeddings = pad_sequence(embeddings, max_len, emb_dim)\n","        \n","    predict_embeddings.append(padded_embeddings)\n","\n","\n","    tensor_emb = torch.tensor(predict_embeddings, dtype= torch.float32)\n","    tensor_emb = tensor_emb.to(device)\n","    \n","    predictions = []\n","    # Perform prediction for hate\n","    with torch.no_grad():\n","        outputs = hate_model(tensor_emb)\n","        proba = F.softmax(outputs,dim=1)\n","        \n","        predictions.append(proba.cpu())\n","    \n","    # perform prediction for targets\n","    with torch.no_grad():\n","        outputs = target_model(tensor_emb)\n","        \n","       \n","        predictions.append(outputs.cpu())\n","        \n","    return predictions\n","    \n","    \n","\n","#  custom input\n","custom_input = \" i hate chineses sunflowers\"\n","\n","# Predict the class for the custom input\n","predict = predict(text = custom_input, hate_model = BiRNN_for_hate_class, target_model = BiRNN_for_target_class, emb_model = pretraines_glove_model)\n","\n","\n","hate_argmax = np.argmax(predict[0][0]).numpy().item()\n","\n","hate_dict = {0: 'normal', 1:'offensive', 2:'hate speech'}\n","\n","hate_label = hate_dict[hate_argmax]\n","hate_label_score = predict[0][0][hate_argmax].numpy().item()\n","\n","\n","multilabel.classes_\n","\n","sigmoid = torch.nn.Sigmoid()\n","probs = sigmoid(predict[1][0].cpu())\n","preds = np.zeros(probs.shape)\n","preds[np.where(probs>=0.3)] = 1\n","\n","target_label = multilabel.inverse_transform(preds.reshape(1,-1))\n","\n","\n","print(f'Hate score: {hate_label}')\n","\n","print(f'Probability: {hate_label_score}')\n","      \n","print(f'Targets: {target_label}')\n","\n","\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
