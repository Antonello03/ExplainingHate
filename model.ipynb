{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonello03/anaconda3/envs/py31/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HateXplain Dataset\n",
    "\n",
    "### Content\n",
    "- **Textual Data**: The dataset contains social media posts from Twitter and Gab.\n",
    "- **Labels**: Each post is labeled as \"normal,\" \"offensive,\" or \"hate,\" indicating the severity of the language.\n",
    "\n",
    "### Annotations\n",
    "- **Label Annotations**: Posts are annotated by multiple human annotators to ensure consistent labeling.\n",
    "- **Rationales**: Annotators provide explanations highlighting specific parts of the text that influenced their labeling decision.\n",
    "- **Target Communities**: Annotations include information on which communities or groups are targeted by the hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
       "        num_rows: 15383\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
       "        num_rows: 1922\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
       "        num_rows: 1924\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"hatexplain\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- I'll classify both the offensiveness in a ordinal fashion\n",
    "- and the targets of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset[\"train\"][\"post_tokens\"]\n",
    "X_train = [\" \".join(sublist) for sublist in X_train]\n",
    "\n",
    "y_train_lbl = dataset[\"train\"][\"annotators\"]\n",
    "y_train_lbl = [el[\"label\"] for el in y_train_lbl]\n",
    "\n",
    "y_train_targets = dataset[\"train\"][\"annotators\"]\n",
    "y_train_targets = [el[\"target\"] for el in y_train_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesson of the week from the holy book the sovereign individual once commerce flows over the internet suppression of speech becomes unstoppable devolution inevitable and the monopoly of violence by governments will end paraphrased from j d davidson rhys mogg\n",
      "[1, 1, 1]\n",
      "[['None'], ['None'], ['None']]\n",
      "\n",
      "watch bbc on muslims abandoning eu <number> syria the father honestly says it bcs they do not like girls being told they have freewill by supposedly educated pple bcs women don t in islam sharia sharialaw realitycheck fgm misogyny hijab rapeculture abuse violence woman women girl girls womansrights womensrights brexit\n",
      "[1, 1, 1]\n",
      "[['Islam', 'Women'], ['Arab', 'Islam', 'Women'], ['Arab', 'Islam']]\n",
      "\n",
      "<user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires\n",
      "[2, 2, 2]\n",
      "[['Homosexual'], ['Homosexual'], ['Homosexual']]\n",
      "\n",
      "the parade needed at least one severed head in honor of muslim culture\n",
      "[0, 2, 0]\n",
      "[['Islam'], ['Islam'], ['Islam']]\n",
      "\n",
      "i look at the mudslime news from europe and think i wish some goat fucker would try that shit down here in louisiana just get within <number> feet of my daughters and thats your life over checkout time mohammed\n",
      "[0, 0, 0]\n",
      "[['Islam'], ['Men', 'Minority', 'Refugee', 'Arab', 'Islam', 'Heterosexual'], ['Refugee', 'Islam']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id1 = 140\n",
    "id2 = 145\n",
    "\n",
    "for el1, el2, el3 in zip(X_train[id1:id2],y_train_lbl[id1:id2],y_train_targets[id1:id2]):\n",
    "    print(el1,el2,el3,sep=\"\\n\",end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will average and normalize the label values. The actual situation is that for every sentence we have three different annotation which can take the 3 values:\n",
    "- 0 - hatespeech\n",
    "- 1 - normal\n",
    "- 2 - offensive\n",
    "\n",
    "I'll first reorder them, so that\n",
    "\n",
    "- 2 = hate\n",
    "- 1 = offensive\n",
    "- 0 = normal\n",
    "\n",
    "than, since for each sentence there are three different annotation, I'll average them and normalize in the range [0, 1]\n",
    "\n",
    "(e.g., \"they playing a lot of ethnic music at this white ass wedding\" has labels [2, 1, 1] -> [1, 0, 0] -> 0.333 -> 0.165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_map = {\n",
    "    0:2,\n",
    "    1:0,\n",
    "    2:1\n",
    "}\n",
    "\n",
    "def avg3(ls):\n",
    "    return (lbl_map[ls[0]]+lbl_map[ls[1]]+lbl_map[ls[2]])/6\n",
    "\n",
    "y_train_lbl = [avg3(x) for x in y_train_lbl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar procedure should be done also for the targets, If two of the three annotators mentioned the same target I'll maintain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done by ChatGPT 4o\n",
    "\n",
    "# Prompt:\n",
    "# Suppose you have three lists of classes,\n",
    "# if the same class is in at least 2 classes\n",
    "# you add it to a new list to return. give me the code\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def find_common_classes(list1, list2, list3):\n",
    "    # Combine all lists into one\n",
    "    combined_list = list1 + list2 + list3\n",
    "    \n",
    "    # Create a counter to count occurrences of each element\n",
    "    counter = Counter(combined_list)\n",
    "    \n",
    "    # Create a list to store elements that appear in at least two of the lists\n",
    "    result = [item for item, count in counter.items() if count >= 2]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_targets = [find_common_classes(trgts[0],trgts[1],trgts[2]) for trgts in y_train_targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample example of the changed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesson of the week from the holy book the sovereign individual once commerce flows over the internet suppression of speech becomes unstoppable devolution inevitable and the monopoly of violence by governments will end paraphrased from j d davidson rhys mogg\n",
      "0.0\n",
      "['None']\n",
      "\n",
      "watch bbc on muslims abandoning eu <number> syria the father honestly says it bcs they do not like girls being told they have freewill by supposedly educated pple bcs women don t in islam sharia sharialaw realitycheck fgm misogyny hijab rapeculture abuse violence woman women girl girls womansrights womensrights brexit\n",
      "0.0\n",
      "['Islam', 'Women', 'Arab']\n",
      "\n",
      "<user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires\n",
      "0.5\n",
      "['Homosexual']\n",
      "\n",
      "the parade needed at least one severed head in honor of muslim culture\n",
      "0.8333333333333334\n",
      "['Islam']\n",
      "\n",
      "i look at the mudslime news from europe and think i wish some goat fucker would try that shit down here in louisiana just get within <number> feet of my daughters and thats your life over checkout time mohammed\n",
      "1.0\n",
      "['Islam', 'Refugee']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id1 = 140\n",
    "id2 = 145\n",
    "\n",
    "for sen, lbl, targets in zip(X_train[id1:id2],y_train_lbl[id1:id2],y_train_targets[id1:id2]):\n",
    "    print(sen,lbl,targets,end=\"\\n\\n\",sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Caucasian', 'Islam', 'Jewish', 'Men', 'Hindu', 'Christian', 'Indigenous', 'Asian', 'Economic', 'Homosexual', 'Indian', 'Hispanic', 'Women', 'African', 'Arab', 'Other', 'Buddhism', 'Refugee', 'Disability', 'None'}\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "classes = set()\n",
    "for ls in y_train_targets:\n",
    "    for el in ls:\n",
    "        classes.add(el)\n",
    "\n",
    "print(classes)\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonello03/anaconda3/envs/py31/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "texts = X_train\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u really think i would not have been raped by feral hindu or muslim back in india or bangladesh and a neo nazi would rape me as well just to see me cry', 'the uk has threatened to return radioactive waste to the eu if an agreement cannot be reached can not we keep the radioactive waste and send back all the paki migrants instead']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1996,  2866,  2038,  5561,  2000,  2709, 17669,  5949,  2000,\n",
       "         1996,  7327,  2065,  2019,  3820,  3685,  2022,  2584,  2064,  2025,\n",
       "         2057,  2562,  1996, 17669,  5949,  1998,  4604,  2067,  2035,  1996,\n",
       "        22190,  2072, 16836,  2612,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonello03/anaconda3/envs/py31/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import gc\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Example data\n",
    "texts = [\"This is a sample sentence.\", \"Each text is one item in the dataset.\", \"Handle larger datasets with DataLoader.\"] * 1000  # Example large dataset\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(texts, tokenizer)\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers = 8)\n",
    "\n",
    "# Store embeddings\n",
    "all_embeddings = []\n",
    "\n",
    "# Process data in batches\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask in data_loader:\n",
    "        # Move input tensors to GPU\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the embeddings for [CLS] token (first token)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu()\n",
    "        all_embeddings.append(embeddings)\n",
    "        \n",
    "        # Free up memory\n",
    "        del input_ids, attention_mask, outputs, embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Concatenate all embeddings\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
